{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr = pd.read_csv(\n",
    "    '../data/raw_csv_files/Amazon Sale Report.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format columns\n",
    "df_amazon_sr.columns = (\n",
    "    df_amazon_sr.columns\n",
    "    .str.strip()                                # Remove leading/trailing whitespace\n",
    "    .str.lower()                                # Convert to lowercase\n",
    "    .str.replace(r'[^\\w\\s]', '', regex=True)    # Remove special characters like - or .\n",
    "    .str.replace(r'\\s+', '_', regex=True)       # Replace spaces with underscores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "cols_to_drop = [\n",
    "    'index',\n",
    "    'asin',\n",
    "    'shipcountry',\n",
    "    'unnamed_22'\n",
    "]\n",
    "df_amazon_sr.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr['sales_channel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 124 rows from non-Amazon sales_channel (all unshipped, not relevant)\n",
    "df_amazon_sr = df_amazon_sr[df_amazon_sr['sales_channel'] == 'Amazon.in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'sales_channel' now that it's uniform\n",
    "\n",
    "# Filter and make a safe copy\n",
    "df_amazon_sr = df_amazon_sr[df_amazon_sr['sales_channel'] == 'Amazon.in'].copy()\n",
    "\n",
    "# Now it's safe to drop the column\n",
    "df_amazon_sr.drop(columns=['sales_channel'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert some data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to datetime64\n",
    "df_amazon_sr['date'] = pd.to_datetime(\n",
    "    df_amazon_sr['date'],\n",
    "    format='%m-%d-%y',     # Specify expected format\n",
    "    errors='coerce'        # Handle bad entries safely\n",
    ")\n",
    "print(df_amazon_sr['date'].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert postal codes to strings so we don't lose any formatting (like leading zeros)\n",
    "df_amazon_sr['shippostalcode'] = (\n",
    "    df_amazon_sr['shippostalcode']\n",
    "    .astype('Int64')   # keep nulls intact\n",
    "    .astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm\n",
    "print(df_amazon_sr[['date', 'shippostalcode']].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show date rage of data\n",
    "print('Date Range:')\n",
    "print('Min:', df_amazon_sr['date'].min())\n",
    "print('Max:', df_amazon_sr['date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In India, Q1 FY23 = April 1, 2022 to June 30, 2022\n",
    "# The original data starts on March 31 and ends on June 29 — likely due to time zone differences or US-based reporting\n",
    "# Shift all dates forward by one day to align cleanly with Indian fiscal Q1 FY23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr['date'] = df_amazon_sr['date'] + pd.Timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show date rage of data\n",
    "print('Date Range:')\n",
    "print('Min:', df_amazon_sr['date'].min())\n",
    "print('Max:', df_amazon_sr['date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the dataframe by date\n",
    "df_amazon_sr = df_amazon_sr.sort_values(by='date').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename some columns for claity\n",
    "df_amazon_sr.rename(columns={\n",
    "    'shipcity': 'city',\n",
    "    'shipstate': 'state',\n",
    "    'shippostalcode': 'postal_code',\n",
    "    'shipservicelevel': 'shipping_method',\n",
    "    'promotionids': 'promo_id',\n",
    "    'fulfilledby': 'fulfilled_by'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Title Case for category values (consistency) & remove any white spaces\n",
    "df_amazon_sr['category'] = df_amazon_sr['category'].str.strip().str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-eheck unique promo_ids\n",
    "df_amazon_sr['promo_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks very messy\n",
    "df_amazon_sr['promo_id'].dropna().unique()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut out everything after 'Free-Financing' (it's meaningless) and assign to new column\n",
    "df_amazon_sr['promo_group'] = df_amazon_sr['promo_id'].str.extract(r'^(.*?Free-Financing)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in all NaNs in promo_group with 'None'\n",
    "df_amazon_sr['promo_group'] = df_amazon_sr['promo_group'].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check value counts\n",
    "df_amazon_sr['promo_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of uniques \n",
    "df_amazon_sr['promo_group'].nunique()\n",
    "# 5787 -> 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original, messy 'promo_id' column\n",
    "df_amazon_sr.drop(columns=['promo_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original column name back; rename 'promo_group' to 'promo_id'\n",
    "df_amazon_sr.rename(columns={'promo_group': 'promo_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in all NaNs in fulfilled_by with 'Other'\n",
    "df_amazon_sr['fulfilled_by'] = df_amazon_sr['fulfilled_by'].fillna('Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up 'city' & 'state' columns now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, format city & state columns to have same Title Case, strip away white space\n",
    "df_amazon_sr['city'] = df_amazon_sr['city'].str.strip().str.title()\n",
    "df_amazon_sr['state'] = df_amazon_sr['state'].str.strip().str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with 'state'\n",
    "df_amazon_sr['state'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View list of unique values\n",
    "sorted(df_amazon_sr['state'].dropna().unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize common spelling errors and abbreviations in 'state'\n",
    "state_corrections = {\n",
    "    'Rajshthan': 'Rajasthan',\n",
    "    'Rajsthan': 'Rajasthan',\n",
    "    'Rj': 'Rajasthan',\n",
    "    'Orissa': 'Odisha',                                             # Official state name\n",
    "    'Pondicherry': 'Puducherry',\n",
    "    'Pb': 'Punjab',\n",
    "    'Punjab/Mohali/Zirakpur': 'Punjab',                             # Keep only state\n",
    "    'New Delhi': 'Delhi',                                           # Normalize NCR variant\n",
    "    'Nl': 'Nagaland',\n",
    "    'Ar': 'Arunachal Pradesh',\n",
    "    'Apo': None,                                                    # Likely invalid\n",
    "    'Dadra And Nagar': 'Dadra And Nagar Haveli And Daman And Diu'   # Official UT name\n",
    "}\n",
    "df_amazon_sr['state'] = df_amazon_sr['state'].replace(state_corrections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr['state'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr['state'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr[df_amazon_sr['state'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropped 34 rows missing both 'city' and 'state' — can't be used for location analysis\n",
    "df_amazon_sr = df_amazon_sr[~(df_amazon_sr['state'].isnull() & df_amazon_sr['city'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "sorted(df_amazon_sr['state'].dropna().unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Check\n",
    "sorted(df_amazon_sr['state'].dropna().unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now 'city'\n",
    "df_amazon_sr['city'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr['postal_code'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_postal = (\n",
    "    df_amazon_sr[['postal_code', 'state', 'city']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_postal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_postal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file (GeoNames, tab-separated, no header)\n",
    "geo_postal = pd.read_csv(\n",
    "    '../data/geonames_india.txt',   # ← update this path\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    dtype={1: str}  # ensure postal_code keeps leading zeros\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_postal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign column names (from the GeoNames readme.txt file)\n",
    "geo_postal.columns = [\n",
    "    'country_code', 'postal_code', 'place_name', 'admin_name1', 'admin_code1',\n",
    "    'admin_name2', 'admin_code2', 'admin_name3', 'admin_code3',\n",
    "    'latitude', 'longitude', 'accuracy'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_postal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_postal.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only rows with valid lat/lon and place_name\n",
    "geo_filtered = geo_postal[\n",
    "    geo_postal['place_name'].notnull() &\n",
    "    geo_postal['latitude'].notnull() &\n",
    "    geo_postal['longitude'].notnull()\n",
    "].copy()\n",
    "\n",
    "# Keep best (lowest) accuracy row for each (postal_code, admin_name1)\n",
    "geo_grouped = (\n",
    "    geo_filtered.sort_values(by='accuracy')\n",
    "    .groupby(['postal_code', 'admin_name1'], as_index=False)\n",
    "    .first()\n",
    ")\n",
    "# Rename columns for merge compatibility\n",
    "geo_grouped.rename(columns={\n",
    "    'place_name': 'city_geo',\n",
    "    'admin_name1': 'state'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if GeoNames format of the state values is = to ours\n",
    "set(geo_grouped['state'].unique()) == set(df_amazon_sr['state'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_amazon_sr['state'].unique()) - set(geo_grouped['state'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match Amazon state names to GeoNames format\n",
    "amazon_state_corrections = {\n",
    "    'Andaman & Nicobar': 'Andaman & Nicobar Islands',\n",
    "    'Chhattisgarh': 'Chattisgarh',\n",
    "    'Dadra And Nagar Haveli And Daman And Diu': 'Dadra and Nagar Haveli and Daman and Diu',\n",
    "    'Ladakh': 'Jammu & Kashmir',\n",
    "    'Puducherry': 'Pondicherry'\n",
    "}\n",
    "df_amazon_sr['state'] = df_amazon_sr['state'].replace(amazon_state_corrections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge enriched city and coordinates into main DataFrame\n",
    "df_amazon_sr = df_amazon_sr.merge(\n",
    "    geo_grouped[['postal_code', 'state', 'city_geo', 'latitude', 'longitude']],\n",
    "    on=['postal_code', 'state'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if GeoNames format of the state values is now = to ours\n",
    "set(geo_grouped['state'].unique()) == set(df_amazon_sr['state'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It checks out, only value is 'None' which geo_grouped does not have\n",
    "set(df_amazon_sr['state'].unique()) - set(geo_grouped['state'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare cleaned 'city' to enriched 'city_geo'\n",
    "df_amazon_sr['city_match'] = (\n",
    "    df_amazon_sr['city'] == df_amazon_sr['city_geo']\n",
    ")\n",
    "\n",
    "# Check match counts\n",
    "df_amazon_sr['city_match'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use city_geo if available, fallback to original city if not\n",
    "df_amazon_sr['city_final'] = df_amazon_sr['city_geo'].fillna(df_amazon_sr['city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr.drop(columns=['city_geo', 'city_match'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check on the structure\n",
    "df_amazon_sr[['city_final', 'state', 'postal_code', 'latitude', 'longitude']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop old city column and replace with trusted GeoNames-based city\n",
    "df_amazon_sr.drop(columns=['city'], inplace=True)\n",
    "df_amazon_sr.rename(columns={'city_final': 'city'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr['city'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for hidden null values\n",
    "df_amazon_sr[df_amazon_sr['city'].isin(['', ' '])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in 'city' and 'state' columns\n",
    "df_amazon_sr[['city', 'state']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate\n",
    "df_amazon_sr[df_amazon_sr['state'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop row with 'Apo' city and missing state — cancelled and not monetized\n",
    "df_amazon_sr = df_amazon_sr[~((df_amazon_sr['city'] == 'Apo') & (df_amazon_sr['state'].isnull()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_amazon_sr['city'].dropna().unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check column order\n",
    "df_amazon_sr.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder column names\n",
    "df_amazon_sr = df_amazon_sr[\n",
    "    [\n",
    "        'date', 'order_id', 'shipping_method', 'status', 'fulfilment', \n",
    "        'courier_status', 'category', 'style', 'sku', 'size', 'qty',\n",
    "        'amount', 'currency', 'city','state', 'postal_code',\n",
    "        'latitude', 'longitude', 'promo_id', 'fulfilled_by', 'b2b'\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(df_amazon_sr.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the 6 pairs of duplicates\n",
    "dupes = df_amazon_sr[df_amazon_sr.duplicated(keep=False)]\n",
    "display(dupes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that they are identical\n",
    "dupes.iloc[0].equals(dupes.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop them and check\n",
    "df_amazon_sr.drop_duplicates(inplace=True)\n",
    "print(df_amazon_sr.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "missing = df_amazon_sr.isnull().sum()\n",
    "missing = missing[missing > 0].sort_values(ascending=False)\n",
    "display(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing lat/lon first\n",
    "df_amazon_sr[df_amazon_sr['latitude'].isnull()][['postal_code', 'state', 'city']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_postals = df_amazon_sr[df_amazon_sr['latitude'].isnull()]['postal_code'].nunique()\n",
    "print(f'Postal codes with missing lat/lon: {missing_postals}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Although 1,172 rows are missing lat/lon, they come from only 222 unique postal codes.\n",
    "# Tableau aggregates by postal code, so these rows collapse into fewer map points.\n",
    "# In Tableau, only 1 unmapped location appears — acceptable for now.\n",
    "# Leaving these rows in the dataset in case we want to patch coordinates later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing amount/currency\n",
    "# Check to see if the 7665 rows with missing amount values were cancelled and that's why they are missing\n",
    "df_amazon_sr[df_amazon_sr['amount'].isnull()]['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most (7559) but not all (7665) were cancelled. 106 non-cancelled orders are missing currency/amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to flag rows that have a valid (non-null) 'amount' value to track which orders contain real payment data\n",
    "df_amazon_sr['has_amount'] = df_amazon_sr['amount'].notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a separate dataframe for completed, paid orders\n",
    "df_amazon_sr_paid = df_amazon_sr[\n",
    "    (df_amazon_sr['amount'].notna()) &\n",
    "    (df_amazon_sr['status'] != 'Cancelled')\n",
    "].copy() # prevent unwanted links to the original dataframe                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing courier_status values\n",
    "# Check how many missing values exist in 'courier_status'\n",
    "df_amazon_sr['courier_status'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if NaN values in courier_status are == status 'Cancelled'\n",
    "df_amazon_sr[df_amazon_sr['courier_status'].isnull()]['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of the 6869 missing courier_status values, 6858 are cancelled. Remaining 11 are in-between or post-ship states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only fill NaNs in courier_status where status is \"Cancelled\", leave the 11 NaNs\n",
    "df_amazon_sr.loc[\n",
    "    (df_amazon_sr['courier_status'].isnull()) & (df_amazon_sr['status'] == 'Cancelled'),\n",
    "    'courier_status'\n",
    "] = 'Unshipped'\n",
    "\n",
    "df_amazon_sr_paid.loc[\n",
    "    (df_amazon_sr_paid['courier_status'].isnull()) & (df_amazon_sr_paid['status'] == 'Cancelled'),\n",
    "    'courier_status'\n",
    "] = 'Unshipped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for suspicious rows: qty == 0 but status is not cancelled\n",
    "df_amazon_sr[(df_amazon_sr['qty'] == 0) & (df_amazon_sr['status'] != 'Cancelled')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with qty == 0 that aren't cancelled (likely invalid)\n",
    "df_amazon_sr = df_amazon_sr[~((df_amazon_sr['qty'] == 0) & (df_amazon_sr['status'] != 'Cancelled'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up 'status' colums values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr['status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all unique 'status' values and how many times each appears\n",
    "df_amazon_sr['status'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 6 rows with problematic shipping statuses — damaged, or lost in transit\n",
    "problematic_statuses = [\n",
    "    'Shipped - Damaged',\n",
    "    'Shipped - Lost in Transit'\n",
    "]\n",
    "\n",
    "df_amazon_sr = df_amazon_sr[\n",
    "    ~df_amazon_sr['status'].isin(problematic_statuses)\n",
    "]\n",
    "df_amazon_sr_paid = df_amazon_sr_paid[\n",
    "    ~df_amazon_sr_paid['status'].isin(problematic_statuses)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simplified 'status_clean' column for grouping and analysis\n",
    "\n",
    "def clean_order_status(status, amount):\n",
    "    if status.startswith('Cancelled'):\n",
    "        return 'Cancelled'\n",
    "    if status.startswith('Pending'):\n",
    "        return 'Pending'\n",
    "    if status in [\n",
    "        'Shipped',\n",
    "        'Shipped - Delivered to Buyer',\n",
    "        'Shipped - Picked Up',\n",
    "        'Shipped - Out for Delivery'\n",
    "    ]:\n",
    "        if amount == 0:\n",
    "            return 'Shipped - Replacement'\n",
    "        return 'Shipped'\n",
    "    if status in [\n",
    "        'Shipped - Returned to Seller',\n",
    "        'Shipped - Rejected by Buyer',\n",
    "        'Shipped - Returning to Seller'\n",
    "    ]:\n",
    "        return 'Returned'\n",
    "    return 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr['status_clean'] = df_amazon_sr.apply(\n",
    "    lambda row: clean_order_status(row['status'], row['amount']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_amazon_sr_paid['status_clean'] = df_amazon_sr_paid.apply(\n",
    "    lambda row: clean_order_status(row['status'], row['amount']),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tab to check how status_clean aligns with courier_status\n",
    "df_amazon_sr.groupby(['status_clean', 'courier_status']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag weird rows with new column status_mismatch\n",
    "df_amazon_sr['status_mismatch'] = (\n",
    "    ((df_amazon_sr['status_clean'] == 'Pending') & (df_amazon_sr['courier_status'] == 'Shipped'))\n",
    ")\n",
    "df_amazon_sr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr_paid.groupby(['status_clean', 'courier_status']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag weird rows with new column status_mismatch\n",
    "df_amazon_sr_paid['status_mismatch'] = (\n",
    "        ((df_amazon_sr_paid['status_clean'] == 'Pending') & (df_amazon_sr_paid['courier_status'] == 'Shipped'))\n",
    ")\n",
    "df_amazon_sr_paid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check both dataframes: \n",
    "# df_amazon_sr should have null values only for currency/amount and lat/lon\n",
    "# df_amazon_sr_paid should only have null for lat/lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr_paid.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning done! Reset the indexes. \n",
    "# Reset the index so it starts at 0 and removes the old index completely\n",
    "df_amazon_sr.reset_index(drop=True, inplace=True)\n",
    "# Reset the index so it starts at 0 and removes the old index completely\n",
    "df_amazon_sr_paid.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the final clean versions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr_paid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr_paid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use df_amazon_sr_paid for financial analysis (real, completed sales only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For SKU-level or category-level trend analysis, sales volume, promo use, and time-based visualizations,\n",
    "# we'll create a new version (df_amazon_sr_mean) with missing 'amount' values filled using the mean price per SKU\n",
    "\n",
    "df_amazon_sr_mean = df_amazon_sr.copy()\n",
    "\n",
    "# Flag rows where 'amount' was originally missing (for transparency in analysis)\n",
    "df_amazon_sr_mean['amount_filled'] = df_amazon_sr_mean['amount'].isnull()\n",
    "\n",
    "# Build a mapping of SKU -> average amount\n",
    "sku_mean_map = (\n",
    "    df_amazon_sr_mean.groupby('sku')['amount']\n",
    "    .mean()\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Fill missing 'amount' values using the SKU-level mean\n",
    "df_amazon_sr_mean['amount'] = df_amazon_sr_mean['amount'].fillna(\n",
    "    df_amazon_sr_mean['sku'].map(sku_mean_map)\n",
    ")\n",
    "\n",
    "# Fill any remaining missing currency values with 'INR'\n",
    "df_amazon_sr_mean['currency'] = df_amazon_sr_mean['currency'].fillna('INR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null values, should be none (except lat/lon)\n",
    "df_amazon_sr_mean.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the 34 skus with null values for amount\n",
    "df_amazon_sr_mean[df_amazon_sr_mean['amount'].isnull()]['sku'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 34 remaining rows where 'amount' could not be calculated (no valid price history for the sku)\n",
    "df_amazon_sr_mean = df_amazon_sr_mean[df_amazon_sr_mean['amount'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check\n",
    "df_amazon_sr_mean.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply same function for the 'status' column to add clean 'order_status' column\n",
    "df_amazon_sr_mean['status_clean'] = df_amazon_sr_mean.apply(\n",
    "    lambda row: clean_order_status(row['status'], row['amount']),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr_paid.groupby(['status_clean', 'courier_status']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag weird rows with new column status_mismatch\n",
    "df_amazon_sr_mean['status_mismatch'] = (\n",
    "    ((df_amazon_sr_mean['status_clean'] == 'Pending') & (df_amazon_sr_mean['courier_status'] == 'Shipped'))\n",
    ")\n",
    "df_amazon_sr_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index so it starts at 0 and removes the old index completely\n",
    "df_amazon_sr_mean.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm shape for each data frame\n",
    "print ('df_amazon_sr',df_amazon_sr.shape)\n",
    "print('df_amazon_sr_paid', df_amazon_sr_paid.shape)\n",
    "print('df_amazon_sr_mean', df_amazon_sr_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the clean dataframes as new .csv files\n",
    "\n",
    "# df_amazon_sr.to_csv('cleaned_data/amazon_sales_clean.csv', index=False)\n",
    "# df_amazon_sr_paid.to_csv('cleaned_data/amazon_sales_paid.csv', index=False)\n",
    "# df_amazon_sr_mean.to_csv('cleaned_data/amazon_sales_mean.csv', index=False)\n",
    "\n",
    "# # index=False keeps the row numbers out of the file for cleaner for loading later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook cleaned the raw Amazon Sales Report CSV and produced three versions:\n",
    "# 1. df_amazon_sr: full cleaned dataset (some included missing 'amount')\n",
    "# 2. df_amazon_sr_paid: contains only valid, completed sales (non-cancelled + 'amount' present (use: financial analysis)\n",
    "# 3. df_amazon_sr_mean: includes rows with filled 'amount'  using mean per sku (use: trend analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr['order_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_sr['order_id'].nunique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
