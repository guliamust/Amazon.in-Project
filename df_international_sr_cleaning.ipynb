{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7130dccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from csv file\n",
    "import os\n",
    "import pandas as pd\n",
    "# Load the Sale Report dataframe if not already loaded for SKU codes\n",
    "df_sale_report = pd.read_csv('./cleaned_data/sale_report_clean.csv')\n",
    "df_international_sr = pd.read_csv('Amazon Data - Capstone Project/International sale Report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aadcdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split df_international_sr into two parts at index 19675\n",
    "# From 19575 the order of columns is different\n",
    "df_int_first_half = df_international_sr.loc[:19674].reset_index(drop=True)\n",
    "df_int_second_half = df_international_sr.loc[19675:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abae563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'index' column\n",
    "df_int_second_half = df_int_second_half.drop(columns=['index'])\n",
    "# Set the first row(19675) as column headers \n",
    "df_int_second_half.columns = df_int_second_half.iloc[0]\n",
    "# Drop the first row which is now redundant\n",
    "df_int_second_half = df_int_second_half.drop(df_int_second_half.index[0]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataframe\n",
    "df_int_first_half.isna().sum(), df_int_second_half.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be53adef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the 'DATE' column to datetime format to detect any not matching values\n",
    "df_int_first_half['date_dt'] = pd.to_datetime(df_int_first_half['DATE'], format='%m-%d-%y', errors='coerce')\n",
    "date_nat = df_int_first_half[df_int_first_half['date_dt'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6508d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Months in the 'Months' column to datetime format to detect any not matching values\n",
    "df_int_first_half['month_dt'] = pd.to_datetime(df_int_first_half['Months'], format='%b-%y', errors='coerce')\n",
    "month_nat = df_int_first_half[df_int_first_half['month_dt'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f0a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataframe\n",
    "df_int_first_half[['month_dt', 'date_dt']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0349e85",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# The rows of date_dt & month_dt with NaT values are the same as CUSTOMER NaN rows \n",
    "# Also, they have no values in other columns. Only the wrong values in DATE or Months columns.\n",
    "(df_int_first_half.loc[df_int_first_half['month_dt'].isna(), 'CUSTOMER'].isna() == df_int_first_half.loc[df_int_first_half['date_dt'].isna(), 'CUSTOMER'].isna()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c1dba2",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Drop rows where 'CUSTOMER' is NaN in df_int_first_half\n",
    "df_int_first_half = df_int_first_half.dropna(subset=['CUSTOMER']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b353eceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int_first_half.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3c2b97",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Uppercase the 'Style' and 'Size' columns in df_int_first_half\n",
    "df_int_first_half['Style'] = df_int_first_half['Style'].str.upper()\n",
    "df_int_first_half['Size'] = df_int_first_half['Size'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a685e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from (Style, Size) to SKU in df_sale_report\n",
    "style_size_to_sku = df_sale_report[['design_no', 'size', 'sku']].dropna().set_index(['design_no', 'size'])['sku'].to_dict()\n",
    "\n",
    "# Fill missing SKUs in df_int_first_half using Style and Size mapping\n",
    "mask_no_sku = df_int_first_half['SKU'].isna()\n",
    "df_int_first_half.loc[mask_no_sku, 'SKU'] = df_int_first_half.loc[mask_no_sku].apply(\n",
    "    lambda row: style_size_to_sku.get((row['Style'], row['Size']), None), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb56557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only consider styles where SKU is NaN in df_int_first_half\n",
    "nan_sku_styles = df_int_first_half.loc[df_int_first_half['SKU'].isna(), 'Style'].dropna().unique()\n",
    "style_to_first_matching_sku = {}\n",
    "\n",
    "for style in nan_sku_styles:\n",
    "    matches = df_sale_report[df_sale_report['sku'].astype(str).str.contains(style, na=False)]\n",
    "    if not matches.empty:\n",
    "        style_to_first_matching_sku[style] = matches.iloc[0]['sku']\n",
    "    else:\n",
    "        style_to_first_matching_sku[style] = None\n",
    "\n",
    "style_to_first_matching_sku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddcd33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For rows in df_int_first_half where SKU is NaN, fill SKU using style_to_first_matching_sku,\n",
    "# but remove the last part after the last '-' and append the Size from df_int_first_half\n",
    "\n",
    "mask_remaining_no_sku = df_int_first_half['SKU'].isna()\n",
    "for idx, row in df_int_first_half[mask_remaining_no_sku].iterrows():\n",
    "    style = row['Style']\n",
    "    size = row['Size']\n",
    "    sku = style_to_first_matching_sku.get(style)\n",
    "    if sku and '-' in sku:\n",
    "        base = '-'.join(sku.split('-')[:-1])\n",
    "        new_sku = f\"{base}-{size}\"\n",
    "        df_int_first_half.at[idx, 'SKU'] = new_sku\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ef61db",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "#NaN SKUs in the second half of the dataframe have no size \n",
    "#We cannot fill them with respective SKU code from df_sale_report\n",
    "df_int_second_half[df_int_second_half['SKU'].isna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53ca4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop last row where 'SKU' is NaN in df_int_first_half\n",
    "df_int_first_half = df_int_first_half.dropna(subset=['SKU']).reset_index(drop=True)\n",
    "df_int_first_half.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ef1278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from (Style, Size) to SKU in Sale Report\n",
    "style_size_to_sku = df_sale_report[['design_no', 'size', 'sku']].dropna().set_index(['design_no', 'size'])['sku'].to_dict()\n",
    "\n",
    "# Fill missing SKUs in df_int_first_half using Style and Size mapping\n",
    "mask_no_sku = df_int_first_half['SKU'].isna()\n",
    "df_int_first_half.loc[mask_no_sku, 'SKU'] = df_int_first_half.loc[mask_no_sku].apply(\n",
    "    lambda row: style_size_to_sku.get((row['Style'], row['Size']), None), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30cdf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the last part of 'SKU' after the last '-' as 'Size' in df_int_second_half\n",
    "df_int_second_half['Size'] = df_int_second_half['SKU'].str.split('-').str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3736f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int_second_half = df_int_second_half.dropna(subset=['SKU']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5401eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int_second_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show columns that are different and same between df_int_first_half and df_int_second_half\n",
    "cols_first = set(df_int_first_half.columns)\n",
    "cols_second = set(df_int_second_half.columns)\n",
    "\n",
    "same_cols = cols_first & cols_second\n",
    "diff_first = cols_first - cols_second\n",
    "diff_second = cols_second - cols_first\n",
    "\n",
    "print(\"Same columns:\", same_cols)\n",
    "print(\"Columns only in first half:\", diff_first)\n",
    "print(\"Columns only in second half:\", diff_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a81288",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Select only the common and necessary columns and reorder them \n",
    "common_cols = ['CUSTOMER', 'DATE', 'Style', 'SKU', 'Size', 'PCS', 'RATE', 'GROSS AMT']\n",
    "\n",
    "df_first_common = df_int_first_half[common_cols]\n",
    "df_second_common = df_int_second_half[common_cols]\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "df_int_clean = pd.concat([df_first_common, df_second_common], ignore_index=True)\n",
    "df_int_clean.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bf5745",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int_clean.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e609e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "shipping_rows = df_int_clean[df_int_clean.apply(lambda row: 'SHIPPING' in row.values, axis=1)]\n",
    "print(shipping_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720a80dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'SHIPPING' appears in the same rows for both 'Style' and 'SKU'\n",
    "shipping_style_idx = df_int_clean[df_int_clean['Style'] == 'SHIPPING'].index\n",
    "shipping_sku_idx = df_int_clean[df_int_clean['SKU'] == 'SHIPPING'].index\n",
    "\n",
    "same_shipping_rows = shipping_style_idx.equals(shipping_sku_idx)\n",
    "same_shipping_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0adfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with Shipping inst4ead of actual values\n",
    "df_int_clean = df_int_clean[df_int_clean['Style'] != 'SHIPPING'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d754f5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change column names to lower case and fix 'gross amt' column name to gross_amt\n",
    "df_int_clean.columns = df_int_clean.columns.str.lower()\n",
    "df_int_clean = df_int_clean.rename(columns={'gross amt': 'gross_amt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5180900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' and 'months' columns in df_int_clean to datetime\n",
    "df_int_clean['date'] = pd.to_datetime(df_int_clean['date'], format='%m-%d-%y', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9c965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 'Category' from df_sale_report for each style in df_int_clean\n",
    "# Create a mapping from style (Design No.) to Category\n",
    "style_to_category = df_sale_report.dropna(subset=['design_no', 'category']).set_index('design_no')['category'].to_dict()\n",
    "\n",
    "# Add a new 'category' column to df_int_clean before 'style'\n",
    "cols = df_int_clean.columns.tolist()\n",
    "insert_at = cols.index('style')\n",
    "df_int_clean.insert(insert_at, 'category', df_int_clean['style'].map(style_to_category))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e147abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show unique styles where category is NaN\n",
    "df_int_clean.loc[df_int_clean['category'].isna(), 'style'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f636ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int_clean[df_int_clean['category'].isna() & df_int_clean['style'].str.contains('CMB5')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f366f4",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Fill category as 'Kurta Set' where category is NaN and sku contains both 'SET' and 'KR'\n",
    "mask = df_int_clean['category'].isna() & df_int_clean['sku'].str.contains('SET') & df_int_clean['sku'].str.contains('KR')\n",
    "df_int_clean.loc[mask, 'category'] = 'Kurta Set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b251b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill category as 'KURTA' where category is NaN and sku contains 'KR'\n",
    "mask_kurta = df_int_clean['category'].isna() & df_int_clean['sku'].str.contains('KR')\n",
    "df_int_clean.loc[mask_kurta, 'category'] = 'Kurta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073bdf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_kurta = df_int_clean['category'].isna() & df_int_clean['sku'].str.contains('SAR086')\n",
    "df_int_clean.loc[mask_kurta, 'category'] = 'Saree'\n",
    "df_int_clean.loc[mask_kurta, 'sku'] = 'SAR086-FREE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35275ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where category is NaN and style is in the specified list\n",
    "drop_styles = [\n",
    "    'TAG PRINTING', 'TAGS(LABOUR)', 'TAGS', 'CMB5',\n",
    "    'LABEL CHARGE', 'SHIPPING CHARGES', 'LABEL MANUF.CHRAGE', 'SAR086'\n",
    "]\n",
    "df_int_clean = df_int_clean[~(df_int_clean['category'].isna() & df_int_clean['style'].isin(drop_styles))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250b43ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand the row with size 'S TO XXL' into 5 rows with sizes S, M, L, XL, XXL\n",
    "sizes = ['S', 'M', 'L', 'XL', 'XXL']\n",
    "\n",
    "# Find all rows in df_int_first_half where Size is 'S TO XXL'\n",
    "mask_multi_size = df_int_clean['size'] == 'S TO XXL'\n",
    "rows_to_expand = df_int_clean[mask_multi_size]\n",
    "\n",
    "expanded_rows = []\n",
    "for _, row in rows_to_expand.iterrows():\n",
    "    # Divide PCS and GROSS AMT by 5 for each size, keep RATE the same\n",
    "    pcs = float(row['pcs']) / 5 if pd.notna(row['pcs']) else None\n",
    "    rate = row['rate']\n",
    "    gross_amt = float(row['gross_amt']) / 5 if pd.notna(row['gross_amt']) else None\n",
    "    for size in sizes:\n",
    "        new_row = row.copy()\n",
    "        new_row['size'] = size\n",
    "        # Change SKU to end with the current size\n",
    "        if pd.notna(row['sku']) and '-' in str(row['sku']):\n",
    "            base_sku = '-'.join(str(row['sku']).split('-')[:-1])\n",
    "            new_row['sku'] = f\"{base_sku}-{size}\"\n",
    "        new_row['pcs'] = f\"{pcs:.2f}\" if pcs is not None else None\n",
    "        new_row['rate'] = rate\n",
    "        new_row['gross_amt'] = f\"{gross_amt:.2f}\" if gross_amt is not None else None\n",
    "        expanded_rows.append(new_row)\n",
    "        new_row['GROSS AMT'] = f\"{gross_amt:.2f}\" if gross_amt is not None else None\n",
    "        expanded_rows.append(new_row)\n",
    "\n",
    "# Remove the original multi-size rows and append the expanded ones\n",
    "df_int_clean = df_int_clean[~mask_multi_size].reset_index(drop=True)\n",
    "df_int_clean = pd.concat([df_int_clean, pd.DataFrame(expanded_rows)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354f05a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how the expansion worked\n",
    "df_int_clean[df_int_clean['style'] == 'JAN8641']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bed83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop fully duplicated rows in df_int_clean\n",
    "df_int_clean = df_int_clean.drop_duplicates().reset_index(drop=True)\n",
    "df_int_clean = df_int_clean.drop(columns=['GROSS AMT'])\n",
    "# Check for missing values in the cleaned dataframe\n",
    "df_int_clean.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695519c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values of relevant columns in df_int_clean to ensure coherence with amazon_sales main CSV\n",
    "columns_to_check = ['customer', 'style', 'sku', 'size', 'category','pcs', 'rate', 'gross_amt']\n",
    "for col in columns_to_check:\n",
    "    print(f\"Unique values in '{col}':\")\n",
    "    print(df_int_clean[col].unique())\n",
    "    print('-' * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c456e1",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Show sizes ending with a dot\n",
    "sizes_with_dot = df_int_clean[df_int_clean['size'].str.endswith('.')]['size'].unique()\n",
    "#print(\"Sizes ending with '.':\", sizes_with_dot)\n",
    "\n",
    "# Remove the dot at the end of 'size'\n",
    "df_int_clean['size'] = df_int_clean['size'].str.rstrip('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29780148",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Replace 'XXXL' with '3XL' in the 'size' column\n",
    "df_int_clean['size'] = df_int_clean['size'].replace('XXXL', '3XL')\n",
    "\n",
    "# Replace 'XXXL' with '3XL' at the end of SKU after the last '-'\n",
    "df_int_clean['sku'] = df_int_clean['sku'].str.replace(r'-(XXXL)$', '-3XL', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582f67bf",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "allowed_sizes = ['XS', 'S', 'M', 'L', 'XL', 'XXL', '3XL', '4XL', '5XL', '6XL', 'FREE']\n",
    "df_int_clean = df_int_clean[df_int_clean['size'].isin(allowed_sizes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e9743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show rows where 'size' is not in the allowed list\n",
    "df_int_clean[~df_int_clean['size'].isin(allowed_sizes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca72065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Capitalize customer names to \"Title Case\" (e.g., \"Gulnara Mustafayeva\")\n",
    "df_int_clean['customer'] = df_int_clean['customer'].str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a4de5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where the 'sku' column value is 'KURTI'\n",
    "df_int_clean = df_int_clean[df_int_clean['sku'] != 'KURTI'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae65ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any missing values\n",
    "df_int_clean = df_int_clean.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e831a45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set category to 'Kurta Set' for rows where style is 'SET350'\n",
    "df_int_clean.loc[df_int_clean['style'] == 'SET350', 'category'] = 'Kurta Set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f17fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find styles present in both df_sale_report and df_int_clean\n",
    "styles_amazon = set(df_sale_report['design_no'].unique())\n",
    "styles_international = set(df_int_clean['style'].unique())\n",
    "\n",
    "# Find common styles\n",
    "common_styles = styles_amazon & styles_international\n",
    "\n",
    "# Create a mapping for style -> category from amazon sales\n",
    "amazon_style_to_category = df_sale_report.dropna(subset=['design_no', 'category']).set_index('design_no')['category'].to_dict()\n",
    "\n",
    "# Create a mapping for style -> category from international sales\n",
    "international_style_to_category = df_int_clean.dropna(subset=['style', 'category']).set_index('style')['category'].to_dict()\n",
    "\n",
    "# Collect mismatches\n",
    "mismatches = []\n",
    "for style in common_styles:\n",
    "    cat_amazon = amazon_style_to_category.get(style)\n",
    "    cat_international = international_style_to_category.get(style)\n",
    "    if cat_amazon and cat_international and cat_amazon != cat_international:\n",
    "        mismatches.append({\n",
    "            'style': style,\n",
    "            'category_amazon': cat_amazon,\n",
    "            'category_international': cat_international\n",
    "        })\n",
    "\n",
    "# Create a DataFrame of mismatches\n",
    "df_category_mismatches = pd.DataFrame(mismatches)\n",
    "df_category_mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9078819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the categories for those styles id int and amazon sales which are not samr into style - category amazon category international\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda5e31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload to schema\n",
    "from dotenv import dotenv_values\n",
    "from sqlalchemy import create_engine, types\n",
    "from sqlalchemy import text\n",
    "\n",
    "my_details = dotenv_values('./.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde1ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_user = my_details.get('pg_user')\n",
    "pg_host = my_details.get('pg_host')\n",
    "pg_port = my_details.get('pg_port')\n",
    "pg_db = my_details.get('pg_db')\n",
    "pg_schema = my_details.get('pg_schema')\n",
    "pg_pass = my_details.get('pg_pass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4666c0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'postgresql://{pg_user}:{pg_pass}@{pg_host}:{pg_port}/{pg_db}'\n",
    "engine = create_engine(url, echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3228251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the CSV file\n",
    "csv_path = './cleaned_data/international_sales_report_clean.csv'\n",
    "df_int_clean.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce0dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned CSV into a DataFrame\n",
    "#csv_path = './cleaned_data/international_sales_report_clean.csv'\n",
    "df_upload = pd.read_csv(csv_path)\n",
    "\n",
    "# Upload to PostgreSQL schema\n",
    "table_name = 'international_clean'\n",
    "df_upload.to_sql(table_name, engine, schema=pg_schema, if_exists='replace', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
